{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "df442061",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from mlxtend.frequent_patterns import apriori\n",
    "from mlxtend.preprocessing import TransactionEncoder\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3ce00a63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving files to: C:\\Users\\User\\Desktop\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Define desktop path for saving files\n",
    "def get_desktop_path():\n",
    "    \"\"\"\n",
    "    Get the path to the user's desktop directory\n",
    "    \"\"\"\n",
    "    desktop_path = os.path.join(os.path.expanduser('~'), 'Desktop')\n",
    "    return desktop_path\n",
    "\n",
    "# Get desktop path\n",
    "DESKTOP_PATH = get_desktop_path()\n",
    "print(f\"Saving files to: {DESKTOP_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5ba5db2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating supermarket transactions...\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Set random seed for reproducible results\n",
    "np.random.seed(42)\n",
    "\n",
    "# Define the pool of supermarket items\n",
    "def generate_item_pool():\n",
    "    \"\"\"\n",
    "    Generate a pool of 30 unique supermarket items\n",
    "    \"\"\"\n",
    "    # Create diverse supermarket item categories\n",
    "    fruits_veggies = ['Apple', 'Banana', 'Orange', 'Tomato', 'Potato', 'Lettuce', 'Carrot']\n",
    "    dairy = ['Milk', 'Cheese', 'Yogurt', 'Butter', 'Eggs']\n",
    "    meat = ['Chicken', 'Beef', 'Fish', 'Pork']\n",
    "    bakery = ['Bread', 'Croissant', 'Bagel', 'Cake']\n",
    "    beverages = ['Coffee', 'Tea', 'Soda', 'Juice', 'Water']\n",
    "    household = ['Soap', 'Shampoo', 'Toothpaste', 'Detergent', 'Toilet Paper']\n",
    "    \n",
    "    # Combine all categories into one pool\n",
    "    item_pool = fruits_veggies + dairy + meat + bakery + beverages + household\n",
    "    return item_pool\n",
    "\n",
    "# Generate simulated transactions\n",
    "def generate_transactions(num_transactions=3000, min_items=2, max_items=7):\n",
    "    \"\"\"\n",
    "    Generate supermarket transactions with random items\n",
    "    \"\"\"\n",
    "    # Get the item pool\n",
    "    items = generate_item_pool()\n",
    "    \n",
    "    #  Initialize list to store transactions\n",
    "    transactions = []\n",
    "    \n",
    "    #  Generate each transaction\n",
    "    for i in range(num_transactions):\n",
    "        # Randomly determine number of items in this transaction\n",
    "        num_items_in_transaction = np.random.randint(min_items, max_items + 1)\n",
    "        \n",
    "        # Randomly select items without replacement\n",
    "        transaction_items = np.random.choice(items, size=num_items_in_transaction, replace=False)\n",
    "        \n",
    "        # Convert to list and sort for consistency\n",
    "        transaction_items = sorted(list(transaction_items))\n",
    "        transactions.append(transaction_items)\n",
    "    \n",
    "    return transactions, items\n",
    "\n",
    "# Generate the transaction data\n",
    "print(\"Generating supermarket transactions...\")\n",
    "transactions, item_pool = generate_transactions(3000)\n",
    "\n",
    "# Create DataFrame and save raw transactions\n",
    "transactions_df = pd.DataFrame({'transaction_id': range(len(transactions)), 'items': transactions})\n",
    "transactions_df.to_csv(os.path.join(DESKTOP_PATH, 'supermarket_transactions.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "17a78110",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 3000 transactions\n",
      "Item pool size: 30 items\n",
      "Sample transaction: [np.str_('Cheese'), np.str_('Croissant'), np.str_('Pork'), np.str_('Toilet Paper'), np.str_('Water')]\n",
      "Applying one-hot encoding...\n",
      "One-hot encoded DataFrame shape: (3000, 30)\n",
      "Columns: [np.str_('Apple'), np.str_('Bagel'), np.str_('Banana'), np.str_('Beef'), np.str_('Bread'), np.str_('Butter'), np.str_('Cake'), np.str_('Carrot'), np.str_('Cheese'), np.str_('Chicken'), np.str_('Coffee'), np.str_('Croissant'), np.str_('Detergent'), np.str_('Eggs'), np.str_('Fish'), np.str_('Juice'), np.str_('Lettuce'), np.str_('Milk'), np.str_('Orange'), np.str_('Pork'), np.str_('Potato'), np.str_('Shampoo'), np.str_('Soap'), np.str_('Soda'), np.str_('Tea'), np.str_('Toilet Paper'), np.str_('Tomato'), np.str_('Toothpaste'), np.str_('Water'), np.str_('Yogurt')]\n",
      "Generating frequent itemsets with Apriori...\n",
      "\n",
      "Top 10 Frequent Itemsets:\n",
      "     support    itemsets  length\n",
      "8   0.157333   (Cheese,)       1\n",
      "0   0.156333    (Apple,)       1\n",
      "24  0.156333      (Tea,)       1\n",
      "20  0.154333   (Potato,)       1\n",
      "9   0.154000  (Chicken,)       1\n",
      "21  0.154000  (Shampoo,)       1\n",
      "23  0.153333     (Soda,)       1\n",
      "1   0.153333    (Bagel,)       1\n",
      "6   0.152333     (Cake,)       1\n",
      "28  0.152333    (Water,)       1\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Display transaction statistics\n",
    "print(f\"Generated {len(transactions)} transactions\")\n",
    "print(f\"Item pool size: {len(item_pool)} items\")\n",
    "print(f\"Sample transaction: {transactions[0]}\")\n",
    "\n",
    "# Preprocessing: One-Hot Encoding\n",
    "def preprocess_transactions(transactions):\n",
    "    \"\"\"\n",
    "    Convert transactions to one-hot encoded format\n",
    "    \"\"\"\n",
    "    # Initialize transaction encoder\n",
    "    te = TransactionEncoder()\n",
    "    \n",
    "    # Fit and transform the transactions\n",
    "    te_array = te.fit(transactions).transform(transactions)\n",
    "    \n",
    "    # Create DataFrame from encoded transactions\n",
    "    one_hot_df = pd.DataFrame(te_array, columns=te.columns_)\n",
    "    \n",
    "    return one_hot_df\n",
    "\n",
    "# Apply one-hot encoding\n",
    "print(\"Applying one-hot encoding...\")\n",
    "one_hot_df = preprocess_transactions(transactions)\n",
    "\n",
    "\n",
    "# Display one-hot encoded data info\n",
    "print(f\"One-hot encoded DataFrame shape: {one_hot_df.shape}\")\n",
    "print(f\"Columns: {list(one_hot_df.columns)}\")\n",
    "\n",
    "# Generate Frequent Itemsets using Apriori\n",
    "def generate_frequent_itemsets(one_hot_df, min_support=0.05):\n",
    "    \"\"\"\n",
    "    Generate frequent itemsets using Apriori algorithm\n",
    "    \"\"\"\n",
    "    # Apply Apriori algorithm with minimum support\n",
    "    frequent_itemsets = apriori(one_hot_df, \n",
    "                               min_support=min_support, \n",
    "                               use_colnames=True)\n",
    "    \n",
    "    # Convert itemsets from frozenset to tuple for easier handling\n",
    "    frequent_itemsets['itemsets'] = frequent_itemsets['itemsets'].apply(lambda x: tuple(x))\n",
    "    \n",
    "    # Add length of itemsets for filtering\n",
    "    frequent_itemsets['length'] = frequent_itemsets['itemsets'].apply(lambda x: len(x))\n",
    "    \n",
    "    # Sort by support in descending order\n",
    "    frequent_itemsets = frequent_itemsets.sort_values('support', ascending=False)\n",
    "    \n",
    "    return frequent_itemsets\n",
    "\n",
    "# Generate frequent itemsets\n",
    "print(\"Generating frequent itemsets with Apriori...\")\n",
    "frequent_itemsets = generate_frequent_itemsets(one_hot_df, min_support=0.05)\n",
    "\n",
    "\n",
    "# Save frequent itemsets to CSV\n",
    "frequent_itemsets.to_csv(os.path.join(DESKTOP_PATH, 'frequent_itemsets.csv'), index=False)\n",
    "\n",
    "# Display top 10 frequent itemsets\n",
    "print(\"\\nTop 10 Frequent Itemsets:\")\n",
    "print(frequent_itemsets.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2763762e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Identifying closed frequent itemsets...\n",
      "\n",
      "Top 10 Closed Frequent Itemsets:\n",
      "     itemsets   support  length\n",
      "0   (Cheese,)  0.157333       1\n",
      "1    (Apple,)  0.156333       1\n",
      "2      (Tea,)  0.156333       1\n",
      "3   (Potato,)  0.154333       1\n",
      "4  (Chicken,)  0.154000       1\n",
      "5  (Shampoo,)  0.154000       1\n",
      "6     (Soda,)  0.153333       1\n",
      "7    (Bagel,)  0.153333       1\n",
      "8     (Cake,)  0.152333       1\n",
      "9    (Water,)  0.152333       1\n"
     ]
    }
   ],
   "source": [
    "# Identify Closed Frequent Itemsets\n",
    "def find_closed_itemsets(frequent_itemsets):\n",
    "    \"\"\"\n",
    "    Identify closed frequent itemsets\n",
    "    Closed itemset: No superset has the same support\n",
    "    \"\"\"\n",
    "    # Convert to list of tuples for easier processing\n",
    "    itemset_list = list(frequent_itemsets[['itemsets', 'support']].itertuples(index=False, name=None))\n",
    "    \n",
    "    # Initialize list for closed itemsets\n",
    "    closed_itemsets = []\n",
    "    \n",
    "    # Check each itemset to see if it's closed\n",
    "    for itemset, support in itemset_list:\n",
    "        is_closed = True\n",
    "        \n",
    "        # Get all supersets of current itemset\n",
    "        for other_itemset, other_support in itemset_list:\n",
    "            # Check if other_itemset is a proper superset\n",
    "            if set(itemset).issubset(set(other_itemset)) and len(other_itemset) > len(itemset):\n",
    "                # If superset has same support, current itemset is not closed\n",
    "                if abs(support - other_support) < 1e-10:  # Account for floating point precision\n",
    "                    is_closed = False\n",
    "                    break\n",
    "        \n",
    "        # If no superset has same support, add to closed itemsets\n",
    "        if is_closed:\n",
    "            closed_itemsets.append((itemset, support))\n",
    "    \n",
    "    # Convert to DataFrame\n",
    "    closed_df = pd.DataFrame(closed_itemsets, columns=['itemsets', 'support'])\n",
    "    closed_df['length'] = closed_df['itemsets'].apply(lambda x: len(x))\n",
    "    closed_df = closed_df.sort_values('support', ascending=False)\n",
    "    \n",
    "    return closed_df\n",
    "\n",
    "# Find closed frequent itemsets\n",
    "print(\"Identifying closed frequent itemsets...\")\n",
    "closed_itemsets = find_closed_itemsets(frequent_itemsets)\n",
    "\n",
    "# Save closed itemsets to CSV\n",
    "closed_itemsets.to_csv(os.path.join(DESKTOP_PATH, 'closed_itemsets.csv'), index=False)\n",
    "\n",
    "# Display top 10 closed itemsets\n",
    "print(\"\\nTop 10 Closed Frequent Itemsets:\")\n",
    "print(closed_itemsets.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "473e1eab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Identifying maximal frequent itemsets...\n",
      "\n",
      "Top 10 Maximal Frequent Itemsets:\n",
      "     itemsets   support  length\n",
      "0   (Cheese,)  0.157333       1\n",
      "1    (Apple,)  0.156333       1\n",
      "2      (Tea,)  0.156333       1\n",
      "3   (Potato,)  0.154333       1\n",
      "4  (Chicken,)  0.154000       1\n",
      "5  (Shampoo,)  0.154000       1\n",
      "6     (Soda,)  0.153333       1\n",
      "7    (Bagel,)  0.153333       1\n",
      "8     (Cake,)  0.152333       1\n",
      "9    (Water,)  0.152333       1\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Identify Maximal Frequent Itemsets\n",
    "def find_maximal_itemsets(frequent_itemsets):\n",
    "    \"\"\"\n",
    "    Identify maximal frequent itemsets\n",
    "    Maximal itemset: No frequent superset exists\n",
    "    \"\"\"\n",
    "    #  Convert to list of tuples\n",
    "    itemset_list = list(frequent_itemsets[['itemsets', 'support']].itertuples(index=False, name=None))\n",
    "    \n",
    "    # Initialize list for maximal itemsets\n",
    "    maximal_itemsets = []\n",
    "    \n",
    "    # Create a set of all frequent itemsets for quick lookup\n",
    "    all_itemsets = set(frequent_itemsets['itemsets'])\n",
    "    \n",
    "    # Check each itemset to see if it's maximal\n",
    "    for itemset, support in itemset_list:\n",
    "        is_maximal = True\n",
    "        \n",
    "        # Check if any superset exists in frequent itemsets\n",
    "        for other_itemset in all_itemsets:\n",
    "            if set(itemset).issubset(set(other_itemset)) and len(other_itemset) > len(itemset):\n",
    "                is_maximal = False\n",
    "                break\n",
    "        \n",
    "        # If no frequent superset exists, add to maximal itemsets\n",
    "        if is_maximal:\n",
    "            maximal_itemsets.append((itemset, support))\n",
    "    \n",
    "    # Convert to DataFrame\n",
    "    maximal_df = pd.DataFrame(maximal_itemsets, columns=['itemsets', 'support'])\n",
    "    maximal_df['length'] = maximal_df['itemsets'].apply(lambda x: len(x))\n",
    "    maximal_df = maximal_df.sort_values('support', ascending=False)\n",
    "    \n",
    "    return maximal_df\n",
    "\n",
    "# Find maximal frequent itemsets\n",
    "print(\"Identifying maximal frequent itemsets...\")\n",
    "maximal_itemsets = find_maximal_itemsets(frequent_itemsets)\n",
    "\n",
    "# Save maximal itemsets to CSV\n",
    "maximal_itemsets.to_csv(os.path.join(DESKTOP_PATH, 'maximal_itemsets.csv'), index=False)\n",
    "\n",
    "# Display top 10 maximal itemsets\n",
    "print(\"\\nTop 10 Maximal Frequent Itemsets:\")\n",
    "print(maximal_itemsets.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "48e101e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "FREQUENT ITEMSETS MINING - SUMMARY STATISTICS\n",
      "==================================================\n",
      "Total Frequent Itemsets: 30\n",
      "Total Closed Itemsets: 30\n",
      "Total Maximal Itemsets: 30\n",
      "\n",
      "Support Range: 0.137 - 0.157\n",
      "Itemset Size Range: 1 - 1\n",
      "\n",
      "Frequent Itemsets by Length:\n",
      "  Length 1: 30 itemsets\n",
      "\n",
      "Top 5 Most Frequent Individual Items:\n",
      "  Cheese: 0.157\n",
      "  Apple: 0.156\n",
      "  Tea: 0.156\n",
      "  Potato: 0.154\n",
      "  Chicken: 0.154\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Analysis and Summary Statistics\n",
    "def generate_summary_statistics(frequent_itemsets, closed_itemsets, maximal_itemsets):\n",
    "    \"\"\"\n",
    "    Generate summary statistics for the analysis\n",
    "    \"\"\"\n",
    "    print(\"=\" * 50)\n",
    "    print(\"FREQUENT ITEMSETS MINING - SUMMARY STATISTICS\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    print(f\"Total Frequent Itemsets: {len(frequent_itemsets)}\")\n",
    "    print(f\"Total Closed Itemsets: {len(closed_itemsets)}\")\n",
    "    print(f\"Total Maximal Itemsets: {len(maximal_itemsets)}\")\n",
    "    \n",
    "    print(f\"\\nSupport Range: {frequent_itemsets['support'].min():.3f} - {frequent_itemsets['support'].max():.3f}\")\n",
    "    print(f\"Itemset Size Range: 1 - {frequent_itemsets['length'].max()}\")\n",
    "    \n",
    "    # Count itemsets by length\n",
    "    print(\"\\nFrequent Itemsets by Length:\")\n",
    "    for length in sorted(frequent_itemsets['length'].unique()):\n",
    "        count = len(frequent_itemsets[frequent_itemsets['length'] == length])\n",
    "        print(f\"  Length {length}: {count} itemsets\")\n",
    "    \n",
    "    # Most frequent individual items\n",
    "    single_items = frequent_itemsets[frequent_itemsets['length'] == 1]\n",
    "    print(f\"\\nTop 5 Most Frequent Individual Items:\")\n",
    "    for _, row in single_items.head().iterrows():\n",
    "        item = list(row['itemsets'])[0]\n",
    "        print(f\"  {item}: {row['support']:.3f}\")\n",
    "\n",
    "#  Generate summary statistics\n",
    "generate_summary_statistics(frequent_itemsets, closed_itemsets, maximal_itemsets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f2ca00e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VALIDATION CHECKS\n",
      " Validation 1 PASSED: All maximal itemsets are closed\n",
      " Validation 2 PASSED: All closed itemsets are frequent\n",
      " Validation 3 PASSED: Maximal itemsets include the largest itemsets\n",
      "\n",
      "Analysis complete. All output files have been generated.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Verification and Validation\n",
    "def validate_results(frequent_itemsets, closed_itemsets, maximal_itemsets):\n",
    "    \"\"\"\n",
    "    Validate that the results make logical sense\n",
    "    \"\"\"\n",
    "    print(\"VALIDATION CHECKS\")\n",
    "    \n",
    "    # Check 1: All maximal itemsets should be closed\n",
    "    maximal_set = set(maximal_itemsets['itemsets'])\n",
    "    closed_set = set(closed_itemsets['itemsets'])\n",
    "    \n",
    "    if maximal_set.issubset(closed_set):\n",
    "        print(\" Validation 1 PASSED: All maximal itemsets are closed\")\n",
    "    else:\n",
    "        print(\" Validation 1 FAILED: Some maximal itemsets are not closed\")\n",
    "    \n",
    "    # Check 2: All closed itemsets should be frequent\n",
    "    frequent_set = set(frequent_itemsets['itemsets'])\n",
    "    \n",
    "    if closed_set.issubset(frequent_set):\n",
    "        print(\" Validation 2 PASSED: All closed itemsets are frequent\")\n",
    "    else:\n",
    "        print(\" Validation 2 FAILED: Some closed itemsets are not frequent\")\n",
    "    \n",
    "    # Check 3: Maximal itemsets should be the largest frequent itemsets\n",
    "    max_length = frequent_itemsets['length'].max()\n",
    "    maximal_max_length = maximal_itemsets['length'].max()\n",
    "    \n",
    "    if max_length == maximal_max_length:\n",
    "        print(\" Validation 3 PASSED: Maximal itemsets include the largest itemsets\")\n",
    "    else:\n",
    "        print(\" Validation 3 FAILED: Maximal itemsets don't include largest itemsets\")\n",
    "\n",
    "# Run validation checks\n",
    "validate_results(frequent_itemsets, closed_itemsets, maximal_itemsets)\n",
    "\n",
    "print(\"\\nAnalysis complete. All output files have been generated.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
